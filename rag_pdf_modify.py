
import os
import tempfile

import streamlit as st
from langchain.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_core.runnables import RunnablePassthrough
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_community.vectorstores import Chroma
from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain.chains import create_history_aware_retriever, create_retrieval_chain
from langchain_core.runnables.history import RunnableWithMessageHistory
from langchain_community.chat_message_histories.streamlit import StreamlitChatMessageHistory
from langchain_core.output_parsers import StrOutputParser
from openai import OpenAI

#from streamlit import caching

#ê±°ì˜ ìµœì¢…ë²„ì „
################### ë°°í¬ ë•Œë¬¸ì— ì¶”ê°€
###https://discuss.streamlit.io/t/chromadb-sqlite3-your-system-has-an-unsupported-version-of-sqlite3/90975

import pysqlite3
import sys
import sqlite3
sys.modules["sqlite3"] = sys.modules.pop("pysqlite3")

#########################
#ì˜¤í”ˆAI API í‚¤ ì„¤ì •
os.environ["OPENAI_API_KEY"] = st.secrets['OPENAI_API_KEY']
id = st.secrets['id']

def cache_clear():
    #st.cache_data.clear()  # Clears @st.cache_data cache
    st.cache_resource.clear()  # Clears @st.cache_resource cache
    #caching.clear_cache()
#cache_resourceë¡œ í•œë²ˆ ì‹¤í–‰í•œ ê²°ê³¼ ìºì‹±í•´ë‘ê¸°
@st.cache_resource
def load_and_split_pdf(file_path):
    loader = PyPDFLoader(file_path)
    return loader.load_and_split()

#í…ìŠ¤íŠ¸ ì²­í¬ë“¤ì„ Chroma ì•ˆì— ì„ë² ë”© ë²¡í„°ë¡œ ì €ì¥


@st.cache_resource
def load_pdf(_file):

    with tempfile.NamedTemporaryFile(mode="wb", delete=False) as tmp_file:
        tmp_file.write(_file.getvalue())
        tmp_file_path = tmp_file.name
        #PDF íŒŒì¼ ì—…ë¡œë“œ
        loader = PyPDFLoader(file_path=tmp_file_path)
        pages = loader.load_and_split()
    return pages

@st.cache_resource
def create_vector_store(_docs):
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)
    split_docs = text_splitter.split_documents(_docs)
    #persist_directory = "./chroma_pdf_db"
    vectorstore = Chroma.from_documents(
        split_docs,
        OpenAIEmbeddings(model='text-embedding-3-small'),
        #   persist_directory=persist_directory
    )
    return vectorstore
#ë§Œì•½ ê¸°ì¡´ì— ì €ì¥í•´ë‘” ChromaDBê°€ ìˆëŠ” ê²½ìš°, ì´ë¥¼ ë¡œë“œ
@st.cache_resource
def get_vectorstore():
    persist_directory = "./chroma_db"
    print(persist_directory)
    #if os.path.exists(persist_directory):
    return Chroma(
        persist_directory=persist_directory,
        embedding_function=OpenAIEmbeddings(model='text-embedding-3-small')
    )
    #else:
     #   return 0

def format_docs(docs):
    return "\n\n".join(doc.page_content for doc in docs)
# PDF ë¬¸ì„œ ë¡œë“œ-ë²¡í„° DB ì €ì¥-ê²€ìƒ‰ê¸°-íˆìŠ¤í† ë¦¬ ëª¨ë‘ í•©ì¹œ Chain êµ¬ì¶•
@st.cache_resource
def initialize_components(selected_model, halu):
    #file_path = r"../data/"
    #file_path = r"C:/Users/Jay/PycharmProjects/test_ai/input3.pdf"
    #pages = load_and_split_pdf(file_path)
    vectorstore = get_vectorstore()
    retriever = vectorstore.as_retriever()

    # ì±„íŒ… íˆìŠ¤í† ë¦¬ ìš”ì•½ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
    contextualize_q_system_prompt = """Given a chat history and the latest user question \
    which might reference context in the chat history, formulate a standalone question \
    which can be understood without the chat history. Do NOT answer the question, \
    just reformulate it if needed and otherwise return it as is."""
    contextualize_q_prompt = ChatPromptTemplate.from_messages(
        [
            ("system", contextualize_q_system_prompt),
            MessagesPlaceholder("history"),
            ("human", "{input}"),
        ]
    )

    # ì§ˆë¬¸-ë‹µë³€ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
    qa_system_prompt = """You are an assistant for question-answering tasks. \
    Use the following pieces of retrieved context to answer the question. \
    If you don't know the answer, say that you don't know. \
    Keep the answer perfect. please use imogi with the answer.
    ëŒ€ë‹µì€ í•œêµ­ì–´ë¡œ í•˜ê³ , ì¡´ëŒ“ë§ì„ ì¨ì¤˜.\
    {context}"""
    qa_prompt = ChatPromptTemplate.from_messages(
        [
            ("system", qa_system_prompt),
            MessagesPlaceholder("history"),
            ("human", "{input}"),
        ]
    )
#í† í°ì œí•œ ì–´ë–»ê²Œí•˜ì§€?
    llm = ChatOpenAI(model=selected_model,temperature=halu)
    #ChatOpenAI(model_name='gpt-4o',
    #    n=1,stop=None, temperature=1.0, api_key=OPENAI_API_KEY)
    #tempê°€ 0ì´ë©´ ê±°ì§“ë§ì„ í•˜ì§€ ì•Šê³  ë¬¸ì„œê¸°ë°˜ìœ¼ë¡œ ë‹µë³€ì„ í•¨
    history_aware_retriever = create_history_aware_retriever(llm, retriever, contextualize_q_prompt)
    question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)
    rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)

    return rag_chain


#retriever = vectorstore.as_retriever(
#    search_type="mmr", search_kwargs={"k": 2, "fetch_k": 10, "lambda_mult": 0.6})
#@st.cache_resource
def initial_not_select(selected_model):
    if "messages" not in st.session_state:
        st.session_state["messages"] = [{"role": "assistant", "content": "ì•ˆë…•í•˜ì„¸ìš” ì±—ë´‡ì…ë‹ˆë‹¤. ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?"}]
    print(st.session_state)
    for msg in st.session_state.messages:
        st.chat_message(msg["role"]).write(msg["content"])
   # prompt=""
    if prompt := st.chat_input(key=1):
        client = OpenAI()
        st.session_state.messages.append({"role": "user", "content": prompt})
        st.chat_message("user").write(prompt)
        response = client.chat.completions.create(model=selected_model, messages=st.session_state.messages)
        msg = response.choices[0].message.content
        st.session_state.messages.append({"role": "assistant", "content": msg})
        st.chat_message("assistant").write(msg)

@st.cache_resource
def chaining(_pages,selected_model,halu):
    vectorstore = create_vector_store(_pages)
    retriever = vectorstore.as_retriever()
    #retriever = vectorstore.as_retriever(search_type="similarity", search_kwargs={"k": 4}) ë™ì¼í•¨
    # ì±„íŒ… íˆìŠ¤í† ë¦¬ ìš”ì•½ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
    contextualize_q_system_prompt = """Given a chat history and the latest user question \
       which might reference context in the chat history, formulate a standalone question \
       which can be understood without the chat history. Do NOT answer the question, \
       just reformulate it if needed and otherwise return it as is."""
    contextualize_q_prompt = ChatPromptTemplate.from_messages(
        [
            ("system", contextualize_q_system_prompt),
            MessagesPlaceholder("history"),
            ("human", "{input}"),
        ]
    )
    #ì´ ë¶€ë¶„ì˜ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ëŠ” ê¸°í˜¸ì— ë”°ë¼ ë³€ê²½í•˜ë©´ ë©ë‹ˆë‹¤.
    qa_system_prompt = """You are an assistant for question-answering tasks. \
    Use the following pieces of retrieved context to answer the question. \
    If you don't know the answer, say that that you don't know. \
    Keep the answer perfect. please use imogi with the answer.
    ëŒ€ë‹µì€ í•œêµ­ì–´ë¡œ í•˜ê³ , ì¡´ëŒ“ë§ì„ ì¨ì¤˜.\
    {context}"""

    qa_prompt = ChatPromptTemplate.from_messages(
        [
            ("system", qa_system_prompt),
            ("human", "{input}"),
        ]
    )
    llm = ChatOpenAI(model=selected_model,temperature=halu)
    #ChatOpenAI(model_name='gpt-4o',
    #    n=1,stop=None, temperature=1.0, api_key=OPENAI_API_KEY)
    #tempê°€ 0ì´ë©´ ê±°ì§“ë§ì„ í•˜ì§€ ì•Šê³  ë¬¸ì„œê¸°ë°˜ìœ¼ë¡œ ë‹µë³€ì„ í•¨
    history_aware_retriever = create_history_aware_retriever(llm, retriever, contextualize_q_prompt)
    question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)
    rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)
    return rag_chain



if "logged_in" not in st.session_state:
    st.session_state["logged_in"] = False

#login

def login(id_in):
    if id == id_in:
        st.session_state["logged_in"] = True
        st.success("login succes")
        st.rerun()
    else:
        st.error(("Wrong ID and Password"))

if not st.session_state["logged_in"]:
    st.title("Login1")
    id_in = st.text_input("ID")
    pw = st.text_input("password", type="password")
    if st.button("Login"):
        cache_clear()
        print(id_in)
        print(id)
        login(id_in)
else:


        # Streamlit UI
    st.header("í•­ê³µí†µì‹ ì†Œ Q&A ì±—ë´‡ ğŸ’¬")
    selection = st.selectbox("ChatGpt,ê¸°ì¡´ Database(ë…¸í•˜ìš° ë“±), PDF ", ("ChatGpt", "Database", "PDF"))
    option = st.selectbox("Select GPT Model", ("gpt-4.1-mini", "gpt-4.1"))
    #st.slider('ëª‡ì‚´ì¸ê°€ìš”?', 0, 130, 25)
    #halu_t = st.slider("ê¸°ì¡´ ë¬¸ì„œë¡œ ë‹µë³€: 0, ì°½ì˜ë ¥ ì¶”ê°€ ë‹µë³€: 1", 0.0,1.0,(0.0))
    halu = st.selectbox("ê¸°ì¡´ ë¬¸ì„œë¡œ ë‹µë³€: 0, ì°½ì˜ë ¥ ì¶”ê°€ ë‹µë³€: 1",("0","0.5","1"))

    #halu= str(halu_t)

    #print(halu_t)
    if selection =="ChatGpt":
        cache_clear()
        initial_not_select(option)

    if selection == "PDF":
        #cache_clear()
        uploaded_file = st.file_uploader("PDF ê¸°ë°˜ ë‹µë³€", type=["pdf"],accept_multiple_files=True)
        for file in uploaded_file:
            pages = load_pdf(file)
            print(pages)
            print(type(pages))
        try:
            rag_chain = chaining(pages, option, halu)
           # print(rag_chain)
            chat_history = StreamlitChatMessageHistory(key="chat_messages")
            if "messages" not in st.session_state:
                st.session_state["messages"] = [{"role": "assistant",
                                                 "content": "ë¬´ì—‡ì´ë“  ë¬¼ì–´!"}]

            conversational_rag_chain = RunnableWithMessageHistory(
                rag_chain,
                lambda session_id: chat_history,
                input_messages_key="input",
                history_messages_key="history",
                output_messages_key="answer",
            )


            for msg in chat_history.messages:
                st.chat_message(msg.type).write(msg.content)

            if prompt_message := st.chat_input("Your question"):
                st.chat_message("human").write(prompt_message)
                with st.chat_message("ai"):
                    with st.spinner("Thinking..."):
                        config = {"configurable": {"session_id": "any"}}
                        response = conversational_rag_chain.invoke(
                            {"input": prompt_message},
                            config)

                        answer = response['answer']
                        st.write(answer)
                        #with st.expander("ì°¸ê³  ë¬¸ì„œ í™•ì¸"):
                         #   for doc in response['context']:
                          #      st.markdown(doc.metadata['source'], help=doc.page_content)
        except:
            st.header("ğŸ“š PDF ì—…ë¡œë“œ í•´ì£¼ì„¸ìš”!")
            st.header("í•œë²ˆì— ì—¬ëŸ¬ íŒŒì¼ ì—…ë¡œë“œ")

    elif selection == "Database":
        cache_clear()
        rag_chain = initialize_components(option, halu)
        chat_history = StreamlitChatMessageHistory(key="chat_messages")


        conversational_rag_chain = RunnableWithMessageHistory(
            rag_chain,
            lambda session_id: chat_history,
            input_messages_key="input",
            history_messages_key="history",
            output_messages_key="answer",
        )
        print(st.session_state)
        if "messages" not in st.session_state:
            st.session_state["messages"] = [{"role": "assistant",
                                             "content": "ë¬´ì—‡ì´ë“  ë¬¼ì–´ë³´ì„¸ìš”!"}]

        for msg in chat_history.messages:
            st.chat_message(msg.type).write(msg.content)

        if prompt_message := st.chat_input("Your question"):
            st.chat_message("human").write(prompt_message)
            with st.chat_message("ai"):
                with st.spinner("Thinking..."):
                    config = {"configurable": {"session_id": "any"}}
                    response = conversational_rag_chain.invoke(
                        {"input": prompt_message},
                        config)

                    answer = response['answer']
                    st.write(answer)
                    #with st.expander("ì°¸ê³  ë¬¸ì„œ í™•ì¸"):
                     #   for doc in response['context']:
                      #      st.markdown(doc.metadata['source'], help=doc.page_content)"""

